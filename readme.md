This is a spider & data cleaning streamlined solution.


# Description
Notice:
- If you want to use this project, you MUST EDIT YOUR OWN STEP #1 (to fetch the specific website).

It contains 3 steps:
- Step 1: fetch(crawl) image links and drop the duplicates.
- Step 2: download images by multiprocessing (multithreading is a fake idea to Python scripts)
- Step 3: clean the images
  - eliminate duplicates by md5
  - calculate the image hash
  - calculate the similarity by the image hash
  - discard the smaller similar images.


# Collaborator
Robin Lan

# And...

If you appreciate my hard work, please give me a star. Thank you.
If you encounter any issues, please feel free to contact me with any questions or concerns.
Thank you for choosing my program and I hope you enjoy this time-saving and enjoyable experience.
